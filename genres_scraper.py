from selenium import webdriver #lets us have browser automated
from selenium.webdriver.chrome.service import Service #specific service for Chrome driver
from webdriver_manager.chrome import ChromeDriverManager #to manage the actual installation
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import random
import csv

# Kept getting blocked :( So we need to rotate user agents (like real users) to make the scraping seem more human-like to avoid detection
# https://proxiesapi.com/articles/rotating-user-agents-in-python-with-ready-to-use-list-in-2023
userAgents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36",
]

def extractGenres(soup):
    """
    Extracts genres from a Goodreads book page soup.
    Takes in soup (BeautifulSoup) as an arg, which is the parsed HTML content of book's page
    """
    genre_section = soup.find("div", {"data-testid": "genresList"}) # need to find where the genres are located in the HTML
    
    if not genre_section:
        print("genres list not found! :( ")  # for debug
        return ["Genre(s) not found!!"]
    
    genres = [genre.get_text(strip=True).lower() for genre in genre_section.find_all("a")] #extract genres by finding all the links (which represent genres)

    if not genres:
        print("No genres were able to be extracted!")  # debug

    return genres if genres else ["Genre(s) not found!!"]

def getTropes(book):
    """    Scrapes the tropes (genres) of a book from Goodreads via selenium to handle JS rendering.."""
    title_author, book_url = book
    print(f"scraping tropes for {title_author}")

    # https://stackoverflow.com/questions/46920243/how-to-configure-chromedriver-to-initiate-chrome-browser-in-headless-mode-throug
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")  #run headless w/o opening a browser window
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={random.choice(userAgents)}") #to randomly select a user agent, as defined above

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options) # actually launches chrome via webdriver

    try:
        #Goes to book URL, waits for genres to appear (manually chose 25s), and randomizes sleep time between requests so we don't get accused of being a bot (oops)
        driver.get(book_url) 
        WebDriverWait(driver, 25).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "[data-testid='genresList']"))
        )
        time.sleep(random.uniform(3, 6))  # more bot detection prevention -- add a random delay 

        soup = BeautifulSoup(driver.page_source, "html.parser")

        # save the extracted HTML so we can debug if necessary (chat, it WAS necessary -- why was finding the genres so annoying)
        with open("debug/debug_selenium.html", "w", encoding="utf-8") as f:
            f.write(soup.prettify())

        # get the actual genres, here we call them "tropes", even though not everything extracted is necessarily a trope
        # will need to filter out tag like "romance" since these are ALL romance, so we don't need that as a "trope" or genre.
        tropes = extractGenres(soup)
        print(f"âœ… Tropes found: {tropes}")

    except Exception as e:
        print(f"error found w/ {book_url}: {e}") #see what the errors are during scraping so we can ~ fix it ~
        tropes = ["no tropes found :( try again?"]

    driver.quit() #closes the browser after scraping!!!!!

    return (title_author, book_url, ", ".join(tropes))

# read the books from file generated by url_scraper.py
books = []
with open("goodreads_bookList.txt", "r") as file: # open saved file with contents of books (author, title, goodreads link)
    for line in file:
        parts = line.rsplit(" - ", 1) #split line up so we can get the URL
        if len(parts) == 2:
            books.append((parts[0].strip(), parts[1].strip()))

# save results to new csv file
with open("books_with_genres.csv", "w", newline="") as file:
    writer = csv.writer(file)
    writer.writerow(["Title+Author", "Link", "Genres/Tropes"])

    for book in books:
        result = getTropes(book)
        writer.writerow(result)
        time.sleep(random.uniform(5, 10))  # more delays to reduce getting blocked!

print(f"Successfully scraped {len(books)} books!") #yay we're done (if this . prints with a good number)
